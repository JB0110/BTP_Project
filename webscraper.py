# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xWgeQ1isYeh0ME1JgaaGm0OhPdkXQDmd
"""

import urllib
import urllib.request, urllib.parse, urllib.error
from urllib.request import Request, urlopen
from bs4 import BeautifulSoup
from googlesearch import search
import re
import nltk
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np
import networkx as nx

def remove_blanks(para):
  lines = para.split("\n")
  non_blank = [line for line in lines if line.strip() != ""]
  targeted_para = ""
  for line in non_blank:
        targeted_para += line + "\n"
  return targeted_para

def scrape(query,start=0,end=10):
  temp = 0
  try:
    for res in search(query,tld="com",lang='en', num=10, start=start, stop=end, pause=2.0):
      req = Request(res, headers={'User-Agent': 'Mozilla/5.0'})
      html = urlopen(req).read()
      soup = BeautifulSoup(html, 'html.parser')
      text = soup.find_all(text = True)
      
      output =""
      
      for t in text:
              if t.parent.name in ['p','h1','h2','h3','h4','h5','h6','title']:
                  output += '{} '.format(t)
      
      output = remove_blanks(output)
      
      try:
        name = soup.title.text
      except:
        name = "Untitled"+ str(temp)
        temp+=1
      
      fname = name+".txt"
      print(fname)

      f = open(fname, 'w')
      f.write(output)
      f = open("summary"+".txt",'a+')
      f.write(output)
  except:
    print("Run again for more documents")

scrape("Parkinson Disease")

file = open("summary.txt", "r")
filedata = file.read()
# print(filedata)

cleanr = re.compile('\\n')
data = re.sub(cleanr,"",filedata)

file.close()

file = open("summary.txt",'w')
file.write(data)
file.close()

def read_article(file_name):
    text = open(file_name, "r")
    filedata = text.readlines()
    paras = filedata[0].split(". ")
    sentences = []

    for sentence in paras:
        #print(sentence)
        sentences.append(sentence.replace("[^a-zA-Z]", " ").split(" "))
    sentences.pop() 
    
    return sentences

def sentence_similarity(para_1, para_2, stopwords=None):
    if stopwords is None:
        stopwords = []
 
    para_1 = [w.lower() for w in para_1]
    para_2 = [w.lower() for w in para_2]
 
    complete_words = list(set(para_1 + para_2))
 
    vector_1 = [0] * len(complete_words)
    vector_2 = [0] * len(complete_words)
 
    #Building vector for the first sentence
    for w in para_1:
        if w in stopwords:
            continue
        vector_1[complete_words.index(w)] += 1
 
    #Building vector for the second sentence
    for w in para_2:
        if w in stopwords:
            continue
        vector_2[complete_words.index(w)] += 1
 
    return 1 - cosine_distance(vector_1, vector_2)
 
def build_similarity_matrix(sentences, stop_words):
    #Creating an empty similarity matrix
    similarity_matrix = np.zeros((len(sentences), len(sentences)))
 
    for i1 in range(len(sentences)):
        for i2 in range(len(sentences)):
            if i1 == i2: #ignore if both are same sentences
                continue 
            similarity_matrix[i1][i2] = sentence_similarity(sentences[i1], sentences[i2], stop_words)

    return similarity_matrix


def print_summary(file_name, top_n=5):
    nltk.download("stopwords")
    stop_words = stopwords.words('english')
    summarized_text = []

    #Reading text and splitting it
    sentences =  read_article(file_name)

    #Generating Similarity Martix across sentences
    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)

    #Ranking sentences in Similarity martix
    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)
    counts = nx.pagerank(sentence_similarity_graph)

    #Sorting the rank and picking top sentences
    ordered_sentences = sorted(((counts[j],t) for j,t in enumerate(sentences)), reverse=True)    
        

    for i in range(top_n):
      summarized_text.append(" ".join(ordered_sentences[i][1]))

    #Printing the summarized text
    with open('summary.txt', 'w') as f:
      for sentence in summarized_text:
        f.write("%s\n" % sentence)
    f.close()

print_summary("summary.txt", 15)

pip install pytube

pip install youtube_search

from pytube import YouTube
from youtube_search import YoutubeSearch

pip install youtube-search

query = input("Enter the name of disease")
results = YoutubeSearch(query, max_results=10).to_dict()
print(results[0]);
##for i in results:

query = input("Enter the name of disease")
results = YoutubeSearch(query, max_results=10).to_dict()

j = 0
for i in results:
  link = input("https://www.youtube.com"+results[j]['url_suffix'])                #getting the link of the particular video to be downloaded.
  yt = YouTube(link)                                                              #object creation using YouTube.
                                                                                  
  # To print title
  print("Title :", yt.title)
  # To get number of views
  print("Views :", yt.views)
  # To get the length of video
  print("Duration :", yt.length)
  # To get description
  print("Description :", yt.description)
  # To get ratings
  print("Ratings :", yt.rating)

  stream = yt.streams.get_highest_resolution()
  stream.download()
  print("Download completed!!")
  j+=1



pip install youtube-transcript-api

#aab inn sabko ek file mai add krna h baki hgya

from youtube_transcript_api import YouTubeTranscriptApi

for vid in results:  
  try:
    sub = YouTubeTranscriptApi.get_transcript(vid['id'])                            #getting transcript/subtitles for the video using get_transcript method
    for i in sub:
      print(i["text"])
  except:
    pass

# pip install threading
pip install instaloader

pip3 install instaloader

pip install instaloader

from google.colab import files
src = list(files.upload().values())[0]
open('engagement.py','wb').write(src)
import engagement

import threading
import instaloader
from instaloader import Instaloader, Profile
import engagement
import pickle

loader = Instaloader()
L = instaloader.Instaloader()

# Login or load session
L.login("baadshahos", "!Y17@Baadshahos")
NUM_POSTS = 10

def get_hashtags_posts(query):
    posts = loader.get_hashtag_posts(query)
    users = {}
    count = 0
    for post in posts:
        profile = post.owner_profile
        if profile.username not in users:
            summary = engagement.get_summary(profile)
            users[profile.username] = summary
            count += 1
            print('{}: {}'.format(count, profile.username))
            if count == NUM_POSTS:
                break
    return users

if __name__ == "__main__":
    hashtag = "tacos"
    users = get_hashtags_posts(hashtag)
    print(users)